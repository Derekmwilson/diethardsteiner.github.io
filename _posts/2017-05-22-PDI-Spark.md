---
layout: post
title: "Pentaho Data Integration: Getting started with the Spark Execution Engine"
summary: This article explains how to configure PDI to run with Spark
date: 2017-05-22
categories: PDI
tags: PDI, Spark
published: false
---  

Download the latest open source PDI version from [here](https://sourceforge.net/projects/pentaho/files/Data%20Integration/7.1/).

Unzip it and move the folder to a convenient location. Make the shell files executable. 

You will also need to have a Hadoop and Spark environment running somewhere (either locally using a VM or Docker or using a minimum local installation as described in my [Setting up a Hadoop Dev Environment for Pentaho Data Integration](http://diethardsteiner.github.io/pdi/2015/06/06/PDI-Hadoop-Dev-Env.html). VMs or Docker image is the easiest way to go, just source them directly from Cloudera, Hortenworks, MapR etc.

Next configure the Big Data Shim, an abstraction layer which enables PDI to work with your Hadoop distro. Details on how to do this are available in various articles online (it's also covered in [Setting up a Hadoop Dev Environment for Pentaho Data Integration](http://diethardsteiner.github.io/pdi/2015/06/06/PDI-Hadoop-Dev-Env.html)).

Start `spoon.sh`.

Create a new sample input file:

```bash
$ cd /tmp
$ echo "banana,2" > test-input.csv
```

Create a new transformation in Spoon. Add a **Text file input** step and configure it to read from this file.

Next on the command line, let's create a dedicated user folder on **HDFS** (adjust to your needs, replace user name dsteiner):

```
hadoop fs -mkdir /user/dsteiner
hadoop fs -chown dsteiner /user/dsteiner
```

Back in **Spoon**, via the **View** tab on the left hand side set up a new **Hadoop clusters** configuration to connect to your cluster of choice. Again, see [Setting up a Hadoop Dev Environment for Pentaho Data Integration](http://diethardsteiner.github.io/pdi/2015/06/06/PDI-Hadoop-Dev-Env.html) for details on how to do this.

Next add a **HDFS file output** step to the canvas and configure it write to the HDFS folder we created earlier on.

We could just **run** the **job** now as it is (and in fact I recommend doing this as it will make sure your HDFS connection details are all correct). Since we want to execute the job via **Spark**, we have to supply the **Spark connection details** as well. 

## Configuring Spark engine for the Adaptive Execution Layer

[Setting up the Adaptive Execution Layer (AEL)](https://help.pentaho.com/Documentation/7.1/0P0/Setting_Up_AEL):

"Your installation of Pentaho 7.1 includes the AEL daemon which you can set up for production to run on your clusters. After you configure the **AEL daemon**, the PDI client connects to the **ZooKeeper** server to communicate with both your **Spark cluster** and the **AEL daemon**, which lives on a node of your cluster to launch and run transformations. For production, you will need to disable the [**Pentaho**] embedded ZooKeeper which ships with the product and set up AEL to use your own ZooKeeper server."

"The **Pentaho Spark application** [Driver] is a subset of PDI used to run PDI transformations with the Spark engine in YARN."

| component | module | environment variable | prod location | description
|-------|---------|----------------------|---------------|------------
| PDI | **AEL Deamon** | `PDI_AEL_DAEMON_HOME` | subset of PDI files required to run with Spark, contained with the **PDI Spark Application/Driver** `data-integration` folder, which has to be deployed onto an edgenode
| PDI | **Spark executor** | `HDFS_SPARK_EXECUTOR_LOCATION` | includes only the required libraries needed by the Spark nodes themselves to execute a transformation
| Pentaho Server | **AEL Deamon** | `PDI_AEL_DAEMON_HOME` | See above

### Pentaho Server AEL Configuration

This step **is not required** for our local dev environment setup and only here to explain how you would set up the Pentaho Server on a separate machine with AEL support:

**Pentaho Server 7.1** has the **AEL daemon** is included. If you are running it on a separate machine, download it from [here](https://sourceforge.net/projects/pentaho/files/Business%20Intelligence%20Server/7.1/) and extract it in a convenient location. Make all shell files in all directories executable. Start the server by running:

```bash
sh start-pentaho.sh
```

Visit the Pentaho web interface on the following URL to make sure everything is working:

```
http://localhost:8080/pentaho/Login
```

It might take some time for the page to load initially.

Next add the **environment variable** `PDI_AEL_DAEMON_HOME` to your `bashrc` or `bash_profile` and point it to `data-integration/adaptive-execution`. Adjust below as required: 

```bash
export PENTAHO_SERVER_HOME=/home/dsteiner/apps/pentaho-server-ce-7.1
export PDI_AEL_DAEMON_HOME=$PENTAHO_SERVER_HOME/data-integration/adaptive-execution
```

Don't forget to source `bashrc`/`bash_profile`.

If you don't have the **Spark client** installed, download `spark-2.1.0-bin-hadoop2.7` from [here](http://spark.apache.org/downloads.html). Just extract it in a convenient location and add `SPARK_HOME` to your `bashrc` or `bash_profile` and point it to the Spark root directory.

### Create PDI Spark Driver

Next we have to bundle all dependencies to run PDI jobs and transformations within Spark **on Yarn** (see also [official docu](https://help.pentaho.com/Documentation/7.1/0P0/Setting_Up_AEL). An **open question** here is if YARN is the only supported resource negotiator.

PDI ships with a command line tool called `spark-app-builder.sh` (find it in the PDI root dir). You can exclude plugins by specifying the `-e` flag and change the output location from the PDI root directory to a custom one by using the `-o` flag:

```bash
export WORK_DIR=/home/dsteiner/apps/
sh spark-app-builder.sh -o $WORK_DIR
```

> **Important**: For some weird reason the folder containing the PDI installation must be named `data-integration`. You cannot change it currently to something else since you will get an error when running this script. I created [this JIRA case](http://jira.pentaho.com/browse/PDI-16325).

This script will create the `pdi-spark-driver.zip` file, which contains a `data-integration` folder and `pdi-spark-executor.zip` file. 

In a **production environment**: 

- you have to copy the `data-integration` folder to the **edge node** where you want to run the **AEL daemon**. Create an environment variable called `PDI_AEL_DAEMON_HOME` to point to this location. 
- copy the `pdi-spark-executor.zip` file to the **HDFS node** where you will run Spark and extract the contents. This folder will be referred to as the variable 'HDFS_SPARK_EXECUTOR_LOCATION'. 

In my case I have a **local dev environment** with Hadoop and Spark natively installed (so no VMs or Docker images) and I configured PDI like so:

- I extracted `pdi-spark-driver.zip` in a convenient location and added the `PDI_AEL_DAEMON_HOME` environment variable to `.bashrc`.
- I uploaded the `pdi-spark-executor.zip` to HDFS. You can theoretically place the file anywhere, but for consistency sake I took a look at the `pmr.kettle.dfs.install.dir` property file in the Big Data shim `plugin.properties` and create a similar folder (see below). Both steps shown below:

```bash
cd $WORK_DIR
unzip pdi-spark-driver.zip -d pdi-ce-7.1-spark-app
cd pdi-ce-7.1-spark-app
hdfs dfs -mkdir -p /opt/pentaho/spark
hdfs dfs -put pdi-spark-executor.zip /opt/pentaho/spark
```

Then I added this to `.bashrc` or `.bash_profile` (amend):

```bash
export PDI_AEL_DAEMON_HOME=/home/dsteiner/apps/pdi-ce-7.1-spark-app/data-integration
export HDFS_SPARK_EXECUTOR_LOCATION=/opt/pentaho/spark
```

Source the file.

### Set Properties in the Setenv Script

"The setenv script runs each time the AEL daemon is started and sets the values for the environment properties. In the `setenv` file, the `SPARK_HOME` and the `SPARK_APP` variable values must be manually entered. Perform the following steps to set these values."

We have to set the `SPARK_HOME` and `HADOOP_HOME` variables in the `setenv` script **in case** they are not already defined system wide (you should really have them define system wide, e.g. in `bashrc`, in which case you do not have to do this step here):

```bash
cd $PDI_AEL_DAEMON_HOME/adaptive-execution
vi setenv
```

> **Note**: There are a lot of Spark related config properties in this file, which you might want to adjust at some point.

### Configure PDI AEL Daemon

The `pdi-daemon` located in `$PDI_AEL_DAEMON_HOME/adaptive-execution` allows you to change the **configuration properties** for the **AEL Deamon**.

You can also disable the **Pentaho Zookeeper**, which is shipped with AEL and instead point to your cluster's Zookeeper.

```bash
cd $PDI_AEL_DAEMON_HOME/adaptive-execution
# see current AEL daemon configuration
./pdi-daemon config --daemon --list
# see current Zookeeper configuration
./pdi-daemon config --zookeeper server --list
# change property value
pdi-daemon config --deamon <property name> <property value>
# enable/disable zookeeper
pdi-daemon config --zookeeper <client|server [enable|disable]>
```

The [official docu](https://help.pentaho.com/Documentation/7.1/0P0/Setting_Up_AEL) has also a dedicated section on **running AEL Daemon with your own Zookeeper Server** - see for more details there.

There is an additional configuration to **run PDI Spark in YARN**:

```bash
cd $PDI_AEL_DAEMON_HOME/adaptive-execution
vi etc/org.pentaho.pdi.engine.daemon.cfg
```
By default the **Spark** is assumed to run in `local` mode (`sparkMaster=local`).

Change the following property values to match this:

```
sparkMaster=yarn
# location of the hadoop config dir
hadoopConfDir=$HADOOP_CONF_DIR
assemblyZip=$HDFS_SPARK_EXECUTOR_LOCATION
```

> **Note**: For my local dev environment I kept `sparkMaster` set to `local`.

> **Note**: Here I set `hadoopConfDir` to my local Hadoop install director config. This is my dev environment. For other environments, you must copy the clusters `*site.xml` files to a convenient location on your machine (chances are that you have already done this when setting up the PDI Hadoop Shim any ways) and point to this location.

There are a lot more config settings, please see [official docu](https://help.pentaho.com/Documentation/7.1/0P0/Setting_Up_AEL).

### Starting the PDI AEL Daemon

Here are the essential commands:

```bash
cd $PDI_AEL_DAEMON_HOME/adaptive-execution
# start daemon
./pdi-daemon start
# start daemon in the foreground / interactively
./pdi-daemon start -i
# stop daemon
./pdi-daemon stop
# show status of daemon
./pdi-daemon status
```

### Checking Setup of Dev Environment

So far we have created following **environment variables** in `.bashrc` (example, adjust paths as required):

```bash
export PENTAHO_SERVER_HOME=/home/dsteiner/apps/pentaho-server-ce-7.1
export PDI_AEL_DAEMON_HOME=/home/dsteiner/apps/pdi-ce-7.1-spark-app/data-integration
export HDFS_SPARK_EXECUTOR_LOCATION=/opt/pentaho/spark
```

We have following **components** installed:

Module | Path
------|--------
PDI   | `/user/dsteiner/home/apps/data-integration`
AEL Daemon | `/home/dsteiner/apps/pdi-ce-7.1-spark-app/data-integration`
Spark executors | `/opt/pentaho/spark`


We have following services running:

- Hadoop
- Spark?
- AEL Daemon
- Spoon


### Create Spark Run Configuration in Spoon

Again, go to the **View** tab and right click on **Run configurations** to set up a new Spark connection (see [the official docu](https://help.pentaho.com/Documentation/7.1/0L0/0Y0/030/030/010) for details). You can set/change various properties (see [here](https://help.pentaho.com/Documentation/7.1/0P0/Setting_Up_AEL#Configurable_Properties) for more details). 

> **Run Configuration Spark**: Note that the **Spark Host URL** is not the web interface URL but the **Zookeeper URL**. These URL depends on how you configured the **AEL Daemon**: This can either be the **AEL Deamon embedded Zookeeper Server** or the **Hadoop cluster Zookeeper Server**.

If you use the AEL Deamon embedded Zookeeper Server, you can see the port by running following command:

```
cd $PDI_AEL_DAEMON_HOME/adaptive-execution
# see current Zookeeper configuration
./pdi-daemon config --zookeeper server --list
```


### Troubleshooting AEL

The [official docu](https://help.pentaho.com/Documentation/7.1/0P0/Setting_Up_AEL/Troubleshooting_AEL) has a section on this topic.
