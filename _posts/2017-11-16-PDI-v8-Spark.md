
---
layout: post
title: "Pentaho Data Integration v8: Getting started with the Spark Execution Engine"
summary: This article explains how to configure PDI to run with Spark
date: 2017-11-16
categories: PDI
tags: PDI, Spark
published: false
---

**Official Documentation**:

- Pentaho v8: [Set Up the Adaptive Execution Layer (AEL)](https://help.pentaho.com/Documentation/8.0/Setup/Configuration/Adaptive_Execution_Layer#Pentaho_Spark_Application)

## Spark Client

If not already installed on your local machine, download the **Spark Client** from [here](http://spark.apache.org/downloads.html).

The **environment variable** `SPARK_HOME` will point to the **Spark Client** folder.


## PDI

PDI Download:

- PDI-CE **v8.0** from [here](https://sourceforge.net/projects/pentaho/files/Pentaho%208.0/client-tools/pdi-ce-8.0.0.0-28.zip/download).

Unzip it and move the folder to a convenient location. Make the shell files executable. 

## AEL Daemon

**Version 8 only**: The **Adaptive Execution Layer** is part of the PDI client now! You can find it in the `adative-execution` folder within `<pdi-client-root>`. Now that's nice ... that's this bit done. 

The **environment variable** `PDI_AEL_DAEMON_HOME` will point to this folder.

> **Note**: There is no dependency any more on **Zookeeper** in v8.
> 
> ## Spark Application
> 
Since some users extend the PDI core functionality by additional steps (plugins) e.g. from the Marketplace, not every PDI setup is the same. We will have to make the essential libraries of our possibly customised PDI setup available to the Spark cluster via building an zip file which bundles any dependencies (libraries). If you've every written a plain Scala or Java Scala app and then created an uber jar, this will sound all quite familiar to you.

**PDI** comes with the **Spark Application Builder Tool**, which we will use to create this zip file (called `pdi-spark-driver.zip`). To be precise, the zip file has two essential contents:

- `data-integration`: This looks pretty much like your local PDI client installation
- `pdi-spark-executor.zip`: bundles all the required PDI libraries, so that it can be distributed on the Spark cluster.


The idea is that you copy `pdi-spark-driver.zip` to an edgenode on your cluster and extract it there. 

If you are just planning to do local development, you can unzip `pdi-spark-driver.zip` locally as well.

And this how your create the zip file on Linux:

```bas
cd <pdi-root>
sh ./spark-app-builder.sh -outputLocation /tmp
```




### Local Setup for Development ONLY

Add following variables to `bashrc` (in example):

```
export HADOOP_CONF_HOME=
export SPARK_HOME=
export PDI_AEL_DAEMON_HOME=
```


Adjust the configuration of the AEL Daemon by adjusting this file: 

```
data-integration/adaptive-execution/config/application.properties
```

Change the following properties:

| property | value          | environment variable 
|----------|----------------|-------------
| `sparkHome` | path your local Spark client | `SPARK_HOME`
| `sparkApp` | path to your AEL Daemon `data-integration` folder | `PDI_AEL_DAEMON_HOME`
| `hadoopConfDir` | path to directory containing all the Hadoop config details (`*-site.xml`) | `HADOOP_CONF_HOME??`


Next create a directory called `kettleConf` inside your **Spark Client**'s root directory:

```bash
cd $SPARK_HOME
mkdir kettleConf
```

Next start the **AEL Daemon** like so:

```bash
cd $PDI_AEL_DAEMON_HOME
sh ./daemon.sh
```

### Running on the Cluster (YARN Mode)

If you intend to use with Spark and YARN on the cluster, the following additional steps are necessary:

1. Copy `pdi-spark-driver.zip` to the **edge node** and unzip the file. As mentioned before, you will find a `data-integration` folder and the `pdi-spark-executor.zip` inside the extracted folder.
2. Copy `pdi-spark-executor.zip` to **HDFS** and extract the contents. This location is referred to as `HDFS_SPARK_EXECUTOR_LOCATION`. [MORE INFO MISSING!!!] 


...

## Securing AEL with Spark

Take a look at the info provided [here](https://help.pentaho.com/Documentation/8.0/Setup/Configuration/Adaptive_Execution_Layer#Pentaho_Spark_Application)